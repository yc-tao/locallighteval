# Default configuration for LocalLightEval
defaults:
  - _self_
  - inference: default
  - data: default
  - output: default
  - mode: null

# Hydra configuration
hydra:
  run:
    dir: outputs/eval/${now:%Y-%m-%d_%H-%M-%S}
  sweep:
    dir: multirun/eval/${now:%Y-%m-%d_%H-%M-%S}
    subdir: ${hydra:job.num}
  job:
    chdir: false

# Model configuration - specify any HuggingFace model or local path
model:
  name: "microsoft/DialoGPT-medium"  # HF repo name or local path
  trust_remote_code: false
  tensor_parallel_size: 1
  dtype: "auto"
  max_model_len: null
  gpu_memory_utilization: 0.9
  visible_devices: null  # e.g., "0,1" or "2" - leave null to use all available GPUs

# Global settings
experiment_name: null
seed: 42
debug: false
dry_run: false
mode: evaluation  # Options: evaluation, summarization, end_to_end

# Summarization settings (used when mode=summarization or mode=end_to_end)
summarization:
  output_suffix: "_summaries"
  extract_patterns: true
  save_original_text: true